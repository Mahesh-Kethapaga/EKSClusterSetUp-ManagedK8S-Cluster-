
EKS Cluster SetUp(Managed K8S-Cluster)
======================================

1. Create IAM Role for EKS Cluster

2. AWS EKS VPC CloudFormation

3. Create EKS Cluster

4. Create IAM Role for EKS Worker Nodes
    A. AmazonEKSWorkerNodePolicy
    B. AmazonEKS_CNI Policy 
    C.AmazonEC2ContainerRegisteyReadOnlyPloicy

5. Create Worker Nodes

6. Create an Instance, Install Kubectl, AWS-Cli, configure AWS,IAM 
    Authenticator( /kube/config) aws eks update-kubeconfig --name 
    democluster --region ap-south-1

*** Note: If i deploy a Cluster-Autoscaler , it has access to K8s API's as well as AWS-API's

*** Note: I am not able to Achieve Cluster-AutoScaler if i use Self-Managed/Bare Metal Cluster.
          But I achieve HPA for that Self-Managed/Bare Metal Cluster

7) ***Cluster AutoScaler Deployment in EKS***

Note: Cluster-AutoSclaer also running as a Pod in K8S-Cluster

 A) Creat AWS Policy with below Actions OR Create an IAM Policy For Worker Node
 ------------------------------------------------------------------------------
 -> After the creation of EKS, The Cluster Autoscaler requires the following
    IAM permissions to make calls to AWS APIs on your behalf.
    I need some AutoScaling Permissions to adjust the desiren number.
   
 -> IAM Policy:
    -----------
	{
	    "Version": "2012-10-17",
	    "Statement": [
		{
		    "Effect": "Allow",
		    "Action": [
		        "autoscaling:DescribeAutoScalingGroups",
		        "autoscaling:DescribeAutoScalingInstances",
		        "autoscaling:DescribeLaunchConfigurations",
		        "autoscaling:DescribeTags",
		        "autoscaling:SetDesiredCapacity",
		        "autoscaling:TerminateInstanceInAutoScalingGroup",
		        "ec2:DescribeLaunchTemplateVersions"
		    ],
		    "Resource": "*"
		}
	    ]
	}

 B) Attach Policy to IAM Role(EKS-WorkerNode-Role) which is used in EKS Node Group(EKS-WorkerNode Group)
 -> If i have attached this Role to the server then ClusterAutoSclaer will be able to communicate with AWS API's

C)***Deploy the Cluster Autoscaler***
  -> Deploy the Cluster Autoscaler to your cluster using this .yaml file
  
 https://raw.githubusercontent.com/kubernetes/autoscaler/master/cluster-autoscaler/cloudprovider/aws/examples/
 cluster-autoscaler-autodiscover.yaml
 
  cluster-autoscaler-autodiscover.yaml
  ------------------------------------
 ---
apiVersion: v1
kind: ServiceAccount
metadata:
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
  name: cluster-autoscaler
  namespace: kube-system
---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRole
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["events", "endpoints"]
    verbs: ["create", "patch"]
  - apiGroups: [""]
    resources: ["pods/eviction"]
    verbs: ["create"]
  - apiGroups: [""]
    resources: ["pods/status"]
    verbs: ["update"]
  - apiGroups: [""]
    resources: ["endpoints"]
    resourceNames: ["cluster-autoscaler"]
    verbs: ["get", "update"]
  - apiGroups: [""]
    resources: ["nodes"]
    verbs: ["watch", "list", "get", "update"]
  - apiGroups: [""]
    resources:
      - "namespaces"
      - "pods"
      - "services"
      - "replicationcontrollers"
      - "persistentvolumeclaims"
      - "persistentvolumes"
    verbs: ["watch", "list", "get"]
  - apiGroups: ["extensions"]
    resources: ["replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["policy"]
    resources: ["poddisruptionbudgets"]
    verbs: ["watch", "list"]
  - apiGroups: ["apps"]
    resources: ["statefulsets", "replicasets", "daemonsets"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["storage.k8s.io"]
    resources: ["storageclasses", "csinodes", "csidrivers", "csistoragecapacities"]
    verbs: ["watch", "list", "get"]
  - apiGroups: ["batch", "extensions"]
    resources: ["jobs"]
    verbs: ["get", "list", "watch", "patch"]
  - apiGroups: ["coordination.k8s.io"]
    resources: ["leases"]
    verbs: ["create"]
  - apiGroups: ["coordination.k8s.io"]
    resourceNames: ["cluster-autoscaler"]
    resources: ["leases"]
    verbs: ["get", "update"]
---
apiVersion: rbac.authorization.k8s.io/v1
kind: Role
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
rules:
  - apiGroups: [""]
    resources: ["configmaps"]
    verbs: ["create", "list", "watch"]
  - apiGroups: [""]
    resources: ["configmaps"]
    resourceNames: ["cluster-autoscaler-status", "cluster-autoscaler-priority-expander"]
    verbs: ["delete", "get", "update", "watch"]

---
apiVersion: rbac.authorization.k8s.io/v1
kind: ClusterRoleBinding
metadata:
  name: cluster-autoscaler
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: ClusterRole
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: rbac.authorization.k8s.io/v1
kind: RoleBinding
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    k8s-addon: cluster-autoscaler.addons.k8s.io
    k8s-app: cluster-autoscaler
roleRef:
  apiGroup: rbac.authorization.k8s.io
  kind: Role
  name: cluster-autoscaler
subjects:
  - kind: ServiceAccount
    name: cluster-autoscaler
    namespace: kube-system

---
apiVersion: apps/v1
kind: Deployment
metadata:
  name: cluster-autoscaler
  namespace: kube-system
  labels:
    app: cluster-autoscaler
spec:
  replicas: 1
  selector:
    matchLabels:
      app: cluster-autoscaler
  template:
    metadata:
      labels:
        app: cluster-autoscaler
      annotations:
        prometheus.io/scrape: 'true'
        prometheus.io/port: '8085'
    spec:
      priorityClassName: system-cluster-critical
      securityContext:
        runAsNonRoot: true
        runAsUser: 65534
        fsGroup: 65534
        seccompProfile:
          type: RuntimeDefault
      serviceAccountName: cluster-autoscaler
      containers:
        - image: registry.k8s.io/autoscaling/cluster-autoscaler:v1.26.2
          name: cluster-autoscaler
          resources:
            limits:
              cpu: 100m
              memory: 600Mi
            requests:
              cpu: 100m
              memory: 600Mi
          command:
            - ./cluster-autoscaler
            - --v=4
            - --stderrthreshold=info
            - --cloud-provider=aws
            - --skip-nodes-with-local-storage=false
            - --expander=least-waste
            - --node-group-auto-discovery=asg:tag=k8s.io/cluster-autoscaler/enabled,k8s.io/cluster-autoscaler/<YOUR CLUSTER NAME> # UPdate Your cluster Name Here..
          env:
            - name: AWS_REGION
              value: YOUR_AWS_REGION # UPdate Your AWS Region Name Here in Which You created the EKS-Cluster
          volumeMounts:
            - name: ssl-certs
              mountPath: /etc/ssl/certs/ca-certificates.crt # /etc/ssl/certs/ca-bundle.crt for Amazon Linux Worker Nodes
              readOnly: true
          imagePullPolicy: "Always"
          securityContext:
            allowPrivilegeEscalation: false
            capabilities:
              drop:
                - ALL
            readOnlyRootFilesystem: true
      volumes:
        - name: ssl-certs
          hostPath:
            path: "/etc/ssl/certs/ca-bundle.crt"
            


# UPdate Your cluster Name Here..
# UPdate Your AWS Region Name Here in Which You created the EKS-Cluster


***Note: We will get some StorageClass configured by default after deploying a Cluster-Autoscaler.

*** Note: If you want create a Stateful Application in your K8S-Cluster
-> an AWS EBS volume to be provisioned by the ebs.csi.aws.com provisioner.  
-> This could mean that the CSI (Container Storage Interface) driver for AWS EBS.

8) *** AWS(EKS) EBS Volume to be provisioned by the ebs.csi.aws.com provisioner *** 

  A) Check if the EBS CSI Driver is Installed:
  --------------------------------------------
  -> $ kubectl get pods -n kube-system

  B) Install the EBS CSI Driver (if not installed):
  --------------------------------------------------
  ** Note: Before Executing following command first Install the Git, if not installed.
  
  -> $ kubectl apply -k "github.com/kubernetes-sigs/aws-ebs-csi-driver/deploy/kubernetes/overlays/stable/ecr/?ref=release-1.11"

  C) Check the StorageClass Configuration:
  --------------------------------------------
***Note: We will get some StorageClass configured by default after deploying a Cluster-Autoscaler.
        
     i) Default StorageClass from Cluster-Autoscaler
        --------------------------------------------
	apiVersion: storage.k8s.io/v1
	kind: StorageClass
	metadata:
	  name: gp2
	provisioner: kubernetes.io/aws-ebs
	parameters:
	  fsType: ext4
	  type: gp2
	reclaimPolicy: Delete
	volumeBindingMode: WaitForFirstConsumer
  
         OR
Note: If Default StorageClass from Cluster-Autoscaler(i.e.. (i) ) 
      is works correctly don't use ii) ii) Use The ebs.csi.aws.com provisioner StorageClass .
             
     ii) Use The ebs.csi.aws.com provisioner StorageClass
     ----------------------------------------------------
     ->Hereâ€™s an example YAML for an EBS StorageClass:	
     
	apiVersion: storage.k8s.io/v1
	kind: StorageClass
	metadata:
	  name: ebs-gp2
	provisioner: ebs.csi.aws.com
	parameters:
	  type: gp2
	  fsType: ext4
	  encrypted: "true"
	reclaimPolicy: Retain
	volumeBindingMode: Immediate
	  
  D) Verify IAM Role Permissions:
  --------------------------------------------
 -> Ensure that the IAM role associated with your EKS worker nodes has the necessary permissions to manage EBS volumes. Attach the following IAM policy to the node role:
----------------------------
  i) Create a policy with the necessary permissions for the EBS CSI driver. Below is an example IAM policy JSON for the EBS CSI driver:
   -----------------------------------------------------------------------------------------------------------------------------------
   EKS_EBS_CSI_Policy.JSON
   ------------------------
	{
	  "Version": "2012-10-17",
	  "Statement": [
		{
		  "Effect": "Allow",
		  "Action": [
		    "ec2:CreateVolume",
		    "ec2:DeleteVolume",
		    "ec2:AttachVolume",
		    "ec2:DetachVolume",
		    "ec2:ModifyVolume",
		    "ec2:DescribeVolumes",
		    "ec2:DescribeVolumeStatus",
		    "ec2:DescribeVolumeAttribute",
		    "ec2:CreateTags",
		    "ec2:DeleteTags",
		    "ec2:DescribeInstances",
		    "ec2:DescribeAvailabilityZones",
		    "ec2:DescribeSecurityGroups",
		    "ec2:DescribeSubnets"
		  ],
		  "Resource": "*"
		}
	  ]
	}
  
   ii)Attach the IAM Policy to the Worker Node Role:
   --------------------------------------------------
   -> Go to Roles and find the role used by your EKS worker nodes and attach EKS_EBS_CSI_Policy.
   
   iii) Verify the EBS CSI Driver is Running:
   -------------------------------------------
   -> List Pods in the kube-system Namespace:
      Ex: $ kubectl get pods -n kube-system -l app=ebs-csi-controller
   -> Check Logs of the EBS CSI Controller:
      Ex: kubectl logs -n kube-system <ebs-csi-controller-pod-name>


9)*** Finally,
   A) Deploy Demo Application to check Nodes Automatically Adjusting by ClusterAutoScaler
   B) Verify the Volumes(PVC) are Provisioning Dynamically or not by Creating Stateful Applications Like MongoDB and Springapp ***  

  Ex: kubectl apply -f mongodbstatefulset.yaml	    ==> For Mongodb ReplicaSet
  Ex: kubectl apply -f springappstatefulset.yaml    ==> To check Springapp internally connected and working with MongoDB statefulset or not				
  
  
  
  


Cluster Capacity
=================
2 Nodes
Each Node 2CPU 8GB
4 CPU
16 GB

Note: I can deploy 500m requested applications 7 to 8.

